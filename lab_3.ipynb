{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi3opoftufOT",
        "outputId": "d4dd3c84-9ea8-48b8-f2ad-ca17d6de07fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'hateful_memes'...\n",
            "remote: Enumerating objects: 9700, done.\u001b[K\n",
            "remote: Counting objects: 100% (9700/9700), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9682/9682), done.\u001b[K\n",
            "remote: Total 9700 (delta 17), reused 9700 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (9700/9700), 1.85 MiB | 4.23 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n",
            "Updating files: 100% (9672/9672), done.\n",
            "Filtering content: 100% (9664/9664), 3.13 GiB | 35.98 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# Download Dataset\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/datasets/neuralcatcher/hateful_memes\n",
        "!cd hateful_memes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Part 1: Text-Only Classification\n",
        "# 3.1 Step 1: Generating Text Embeddings\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load BERT tokenizer and model from huggingface\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode since we won ’t train it\n",
        "model.eval()\n",
        "\n",
        "# Example text\n",
        "texts = [\"This is the first sentence.\", \"This is the second sentence.\"]\n",
        "\n",
        "# Tokenize and encode the text inputs\n",
        "inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Pass the inputs through the BERT model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract the sentence embeddings\n",
        "sentence_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "print(sentence_embeddings.shape)\n",
        "\n",
        "# Save embeddings to a file\n",
        "torch.save(sentence_embeddings, \"sentence_embeddings.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDZT_5Voicqe",
        "outputId": "2f4c14b7-3089-43e1-95bd-00691efac689"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1657617de945>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_embeddings = torch.load(\"sentence_embeddings.pt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 Step 2: Building the Classifier\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple fully connected neural network\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    # Forward pass through the network\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = 768\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "\n",
        "# Create the untrained model based on this architecture\n",
        "model = TextClassifier(input_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "DsAHzEAmkJiZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3 Step 3: Training and Evaluation\n",
        "# Dev set results\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Number of epochs for training\n",
        "num_epochs = 10\n",
        "\n",
        "# Example data\n",
        "sentence_embeddings = torch.randn(100, 768)\n",
        "labels = torch.randint(0, 2, (100, 1), dtype=torch.float32)\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(sentence_embeddings)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss after each epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Set the model to evaluation mode for evaluation\n",
        "model.eval()\n",
        "\n",
        "# Evaluation on the dev set (use dev set data here)\n",
        "with torch.no_grad():\n",
        "    dev_outputs = model(sentence_embeddings)\n",
        "\n",
        "    # Apply sigmoid to convert logits to probabilities and round to get predictions (0 or 1)\n",
        "    dev_preds = torch.sigmoid(dev_outputs).round()\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(labels, dev_preds)\n",
        "precision = precision_score(labels, dev_preds)\n",
        "recall = recall_score(labels, dev_preds)\n",
        "f1 = f1_score(labels, dev_preds)\n",
        "auc_roc = roc_auc_score(labels, torch.sigmoid(dev_outputs))\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"Text Model - Dev Set Results:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\\nAUC-ROC: {auc_roc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaSx6PV2lH8X",
        "outputId": "9bedcb69-d649-4f02-e5f5-026085df2225"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.7102136015892029\n",
            "Epoch 2/10, Loss: 0.5504420399665833\n",
            "Epoch 3/10, Loss: 0.42620375752449036\n",
            "Epoch 4/10, Loss: 0.328424870967865\n",
            "Epoch 5/10, Loss: 0.2517696022987366\n",
            "Epoch 6/10, Loss: 0.1923561990261078\n",
            "Epoch 7/10, Loss: 0.1465851217508316\n",
            "Epoch 8/10, Loss: 0.11161170899868011\n",
            "Epoch 9/10, Loss: 0.08500272780656815\n",
            "Epoch 10/10, Loss: 0.06489279121160507\n",
            "Text Model - Dev Set Results:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "AUC-ROC: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model after training\n",
        "torch.save(model.state_dict(), 'text_classifier.pth')"
      ],
      "metadata": {
        "id": "1_87yqADlOKd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load text embeddings\n",
        "text_embeddings_path = '/content/sentence_embeddings.pt'\n",
        "test_text_embeddings = torch.load(text_embeddings_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vexnKgtkcDWL",
        "outputId": "970dbb95-6058-4382-aa22-3d5824bb41ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a04e0285958f>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_text_embeddings = torch.load(text_embeddings_path) # Load with torch.load instead of np.load\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text-only / Test set results\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "test_text_embeddings = torch.randn(100, 768)\n",
        "test_labels = torch.randint(0, 2, (100, 1), dtype=torch.float32)\n",
        "\n",
        "# Load the trained model\n",
        "text_model = TextClassifier(input_dim=768, hidden_dim=256, output_dim=1)\n",
        "text_model.load_state_dict(torch.load('/content/text_classifier.pth'))\n",
        "\n",
        "\n",
        "# Set model to evaluation mode\n",
        "text_model.eval()\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    test_text_outputs = text_model(test_text_embeddings)\n",
        "    test_text_preds = torch.sigmoid(test_text_outputs).round().squeeze()\n",
        "\n",
        "# Convert to numpy for sklearn metrics\n",
        "test_text_preds = test_text_preds.cpu().numpy()\n",
        "test_labels_np = test_labels.squeeze().cpu().numpy()\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "accuracy = accuracy_score(test_labels_np, test_text_preds)\n",
        "precision = precision_score(test_labels_np, test_text_preds)\n",
        "recall = recall_score(test_labels_np, test_text_preds)\n",
        "f1 = f1_score(test_labels_np, test_text_preds)\n",
        "auc_roc = roc_auc_score(test_labels_np, torch.sigmoid(test_text_outputs).cpu().numpy())\n",
        "\n",
        "print(f\"Text Model - Test Set Results:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\\nAUC-ROC: {auc_roc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqFM9gO1cqJR",
        "outputId": "b7167953-5200-42b7-e3ce-e1ea42405ebf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Model - Test Set Results:\n",
            "Accuracy: 0.57\n",
            "Precision: 0.525\n",
            "Recall: 0.4666666666666667\n",
            "F1-Score: 0.49411764705882355\n",
            "AUC-ROC: 0.5595959595959596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-c4712508d449>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  text_model.load_state_dict(torch.load('/content/text_classifier.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Part 2: Image-Only Classification\n",
        "# 4.1 Step 1: Generating image embeddings\n",
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define the preprocessing pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize with the mean and std values used for ResNet training\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Path to the folder containing images\n",
        "image_folder = '/content/hateful_memes/img'\n",
        "\n",
        "# List to store the processed images\n",
        "processed_images = []\n",
        "\n",
        "# Iterate through all files in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "        # Construct the full path to the image\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Open the image using PIL\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        # Apply the transformations\n",
        "        image = transform(image)\n",
        "\n",
        "        # Add the processed image to the list\n",
        "        processed_images.append(image)"
      ],
      "metadata": {
        "id": "c8jvlISgluZ9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Add a batch dimension\n",
        "image = image.unsqueeze(0)\n",
        "\n",
        "# Load the pretrained ResNet18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Remove the final classification layer to get the raw feature vector (embedding)\n",
        "resnet_feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "# Pass the image through ResNet to get the feature representation\n",
        "with torch.no_grad():\n",
        "    image_representation = resnet_feature_extractor(image)\n",
        "\n",
        "# Reshape the output from [1, 512, 1, 1] to [1, 512] to get a 512-dimensional feature vector\n",
        "image_representation = image_representation.view(image_representation.size(0), -1)  # Shape: [1, 512]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiDWjReUoFik",
        "outputId": "ed317d9a-ce5f-433d-ee91-ac99662c700b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 53.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the embeddings to a file\n",
        "torch.save(image_representation, 'image_representation.pth')"
      ],
      "metadata": {
        "id": "7EvYYXFLoIsL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2 Step 2: Building the Classifier\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple fully connected neural network\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ImageClassifier, self).__init__()\n",
        "\n",
        "        # First fully connected layer\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Second fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)  # First layer\n",
        "        x = self.relu(x)  # ReLU activation\n",
        "        x = self.fc2(x)  # Second layer\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = 512\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "\n",
        "# Create the untrained model\n",
        "model = ImageClassifier(input_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "xRFD6lgHoM7Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.3 Step 3: Training and Evaluation\n",
        "# Dev set results\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Number of epochs for training\n",
        "num_epochs = 10\n",
        "\n",
        "# Example data\n",
        "image_embeddings = torch.randn(100, 768)\n",
        "labels = torch.randint(0, 2, (100, 1), dtype=torch.float32)\n",
        "\n",
        "# Adjust the input dimension of the model to match the image_embeddings shape\n",
        "# Create a new model with the correct input dimension\n",
        "input_dim = 768\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "model = ImageClassifier(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(image_embeddings)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss after each epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Set the model to evaluation mode for evaluation\n",
        "model.eval()\n",
        "\n",
        "# Evaluation on the dev set\n",
        "with torch.no_grad():\n",
        "    dev_outputs = model(image_embeddings)\n",
        "\n",
        "    # Apply sigmoid to convert logits to probabilities and round to get predictions (0 or 1)\n",
        "    dev_preds = torch.sigmoid(dev_outputs).round()\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(labels, dev_preds)\n",
        "precision = precision_score(labels, dev_preds)\n",
        "recall = recall_score(labels, dev_preds)\n",
        "f1 = f1_score(labels, dev_preds)\n",
        "auc_roc = roc_auc_score(labels, torch.sigmoid(dev_outputs))\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"Image Model - Dev Set Results:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\\nAUC-ROC: {auc_roc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgmEA9Iy8byQ",
        "outputId": "1d39ad62-05cd-43fe-c2ee-042780841746"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.7023346424102783\n",
            "Epoch 2/10, Loss: 0.7023346424102783\n",
            "Epoch 3/10, Loss: 0.7023346424102783\n",
            "Epoch 4/10, Loss: 0.7023346424102783\n",
            "Epoch 5/10, Loss: 0.7023346424102783\n",
            "Epoch 6/10, Loss: 0.7023346424102783\n",
            "Epoch 7/10, Loss: 0.7023346424102783\n",
            "Epoch 8/10, Loss: 0.7023346424102783\n",
            "Epoch 9/10, Loss: 0.7023346424102783\n",
            "Epoch 10/10, Loss: 0.7023346424102783\n",
            "Image Model - Dev Set Results:\n",
            "Accuracy: 0.5\n",
            "Precision: 0.4942528735632184\n",
            "Recall: 0.8775510204081632\n",
            "F1-Score: 0.6323529411764706\n",
            "AUC-ROC: 0.5214085634253701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the embeddings to a file\n",
        "torch.save(model.state_dict(), 'image_classifier.pth')"
      ],
      "metadata": {
        "id": "2VXFkAtP9HcL"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image embeddings\n",
        "image_embeddings_path = '/content/image_representation.pth'\n",
        "test_image_embeddings = torch.load(image_embeddings_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVok543ddx3H",
        "outputId": "709301c9-e30a-4574-a098-a5739ae4d935"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-fd457556dccc>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_image_embeddings = torch.load(image_embeddings_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image-only / Test set results\n",
        "\n",
        "test_image_embeddings = torch.randn(100, 512)  # Replace with actual test image embeddings\n",
        "\n",
        "# Set model to evaluation mode\n",
        "image_model.eval()\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    test_image_outputs = image_model(test_image_embeddings)\n",
        "    test_image_preds = torch.sigmoid(test_image_outputs).round().squeeze()\n",
        "\n",
        "# Convert to numpy for sklearn metrics\n",
        "test_image_preds = test_image_preds.cpu().numpy()\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "accuracy = accuracy_score(test_labels_np, test_image_preds)\n",
        "precision = precision_score(test_labels_np, test_image_preds)\n",
        "recall = recall_score(test_labels_np, test_image_preds)\n",
        "f1 = f1_score(test_labels_np, test_image_preds)\n",
        "auc_roc = roc_auc_score(test_labels_np, torch.sigmoid(test_image_outputs).cpu().numpy())\n",
        "\n",
        "print(f\"Image Model - Test Set Results:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\\nAUC-ROC: {auc_roc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI38L5wRkAGK",
        "outputId": "9db486a5-a4cb-411e-9bf9-6e899cd9fd73"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Model - Test Set Results:\n",
            "Accuracy: 0.48\n",
            "Precision: 0.4507042253521127\n",
            "Recall: 0.7111111111111111\n",
            "F1-Score: 0.5517241379310345\n",
            "AUC-ROC: 0.534949494949495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Part 3: Multimodal Classification\n",
        "# 5.1 Early Fusion (Feature-Level Fusion)\n",
        "# 5.1.1 Step 1: Extract Text and Image Features\n",
        "\n",
        "# Loading the saved text and image features\n",
        "text_features = torch.load('/content/sentence_embeddings.pt')\n",
        "image_features = torch.load('/content/image_representation.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnkD1MdJ-Eid",
        "outputId": "575cc8c7-9295-4f4e-dc7a-58c058f70416"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-84-4b25737f18bf>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  text_features = torch.load('/content/sentence_embeddings.pt')  # Shape: [num_samples, text_feature_size]\n",
            "<ipython-input-84-4b25737f18bf>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  image_features = torch.load('/content/image_representation.pth')  # Shape: [num_samples, image_feature_size]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1.2 Step 2: Concatenate Features and Train\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Check if the number of samples is the same for all tensors\n",
        "num_samples = min(text_features.shape[0], image_features.shape[0], labels.shape[0])\n",
        "\n",
        "# Slice the tensors to ensure they have the same number of samples\n",
        "text_features = text_features[:num_samples]\n",
        "image_features = image_features[:num_samples]\n",
        "labels = labels[:num_samples]\n",
        "\n",
        "dataset = TensorDataset(text_features, image_features, labels)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Example training loop for the multimodal classifier\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for text_batch, image_batch, labels_batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get features from pre-extracted text and image features\n",
        "        labels = labels_batch.long()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(text_batch, image_batch)\n",
        "\n",
        "        # Compute loss and backpropagate\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URgiE6Y3-r3V",
        "outputId": "eb139b44-b060-4013-c503-9adf5dfe79dc"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.47857218980789185\n",
            "Epoch 2, Loss: 0.006389427464455366\n",
            "Epoch 3, Loss: 0.00044764988706447184\n",
            "Epoch 4, Loss: 5.054346183896996e-05\n",
            "Epoch 5, Loss: 7.748573807475623e-06\n",
            "Epoch 6, Loss: 1.4305104514278355e-06\n",
            "Epoch 7, Loss: 3.576278118089249e-07\n",
            "Epoch 8, Loss: 1.1920928244535389e-07\n",
            "Epoch 9, Loss: 0.0\n",
            "Epoch 10, Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Early fusion training and evaluation\n",
        "# Dev set results\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Define the multimodal model for early fusion (concatenation of text and image features)\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, text_input_size, image_input_size, hidden_size=256):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(text_input_size + image_input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
        "        x = self.fc1(combined_features)\n",
        "        x = self.relu(x)\n",
        "        output = self.fc2(x)\n",
        "        return output\n",
        "\n",
        "# Define the model\n",
        "text_input_size = 768\n",
        "image_input_size = 512\n",
        "model = MultimodalClassifier(text_input_size, image_input_size)\n",
        "\n",
        "# Example batch of features\n",
        "text_features = torch.randn(32, 768)\n",
        "image_features = torch.randn(32, 512)\n",
        "dev_labels = torch.randint(0, 2, (32,))\n",
        "\n",
        "# Forward pass through the multimodal model\n",
        "combined_output = model(text_features, image_features)\n",
        "\n",
        "# Convert logits to binary predictions using sigmoid and rounding\n",
        "dev_preds = torch.sigmoid(combined_output).round().squeeze().detach().cpu().numpy()\n",
        "\n",
        "# Convert dev_labels to numpy array and flatten\n",
        "dev_labels_np = dev_labels.squeeze().cpu().numpy()\n",
        "\n",
        "# Check for shape mismatch and print the shapes\n",
        "if dev_labels_np.shape != dev_preds.shape:\n",
        "    print(f\"Shape mismatch: dev_labels shape {dev_labels_np.shape}, dev_preds shape {dev_preds.shape}\")\n",
        "else:\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(dev_labels_np, dev_preds)\n",
        "    precision = precision_score(dev_labels_np, dev_preds)\n",
        "    recall = recall_score(dev_labels_np, dev_preds)\n",
        "    f1 = f1_score(dev_labels_np, dev_preds)\n",
        "    auc_roc = roc_auc_score(dev_labels_np, torch.sigmoid(combined_output).detach().cpu().numpy().flatten())\n",
        "\n",
        "print(f\"Early Fusion Model - Dev Set Results:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\\nAUC-ROC: {auc_roc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Di0ayZwL_pV",
        "outputId": "0cb50595-cf96-4d3c-c20b-f1b5981b6631"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early Fusion Model - Dev Set Results:\n",
            "Accuracy: 0.46875\n",
            "Precision: 0.5\n",
            "Recall: 0.8823529411764706\n",
            "F1-Score: 0.6382978723404256\n",
            "AUC-ROC: 0.5058823529411764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Early fusion / Test set results\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    # Pass text and image features separately to the model\n",
        "    test_combined_output = model(test_text_embeddings, test_image_embeddings)\n",
        "    test_combined_preds = torch.sigmoid(test_combined_output).round().squeeze()\n",
        "\n",
        "# Convert to numpy for sklearn metrics\n",
        "test_combined_preds = test_combined_preds.cpu().numpy()\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "accuracy = accuracy_score(test_labels_np, test_combined_preds)\n",
        "precision = precision_score(test_labels_np, test_combined_preds)\n",
        "recall = recall_score(test_labels_np, test_combined_preds)\n",
        "f1 = f1_score(test_labels_np, test_combined_preds)\n",
        "auc_roc = roc_auc_score(test_labels_np, torch.sigmoid(test_combined_output).cpu().numpy())\n",
        "\n",
        "print(f\"Early Fusion Model - Test Set Results:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\\nAUC-ROC: {auc_roc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaEkS5UafP3Q",
        "outputId": "35386429-e9a0-4334-e9ff-9f59f92bb1b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early Fusion Model - Test Set Results:\n",
            "Accuracy: 0.56\n",
            "Precision: 0.6078431372549019\n",
            "Recall: 0.5636363636363636\n",
            "F1-Score: 0.5849056603773585\n",
            "AUC-ROC: 0.5931313131313132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.2 Late Fusion (Decision-Level Fusion)\n",
        "# Late fusion training and evaluation\n",
        "# Dev set results\n",
        "\n",
        "import torch\n",
        "\n",
        "# Example outputs for batch size 32\n",
        "batch_size = 32\n",
        "text_logits = torch.randn(batch_size, 1)\n",
        "image_logits = torch.randn(batch_size, 1)\n",
        "\n",
        "# Combine the logits (late fusion)\n",
        "combined_logits = (text_logits + image_logits) / 2\n",
        "\n",
        "# Convert combined logits to predicted classes (for binary classification)\n",
        "predictions = torch.sigmoid(combined_logits).round()\n",
        "\n",
        "# Example ground truth labels (binary classification: 0 or 1)\n",
        "dev_labels = torch.randint(0, 2, (batch_size, 1))\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Convert tensors to numpy arrays for evaluation\n",
        "dev_labels_np = dev_labels.numpy()\n",
        "predictions_np = predictions.numpy()\n",
        "\n",
        "accuracy = accuracy_score(dev_labels_np, predictions_np)\n",
        "precision = precision_score(dev_labels_np, predictions_np)\n",
        "recall = recall_score(dev_labels_np, predictions_np)\n",
        "f1 = f1_score(dev_labels_np, predictions_np)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Late Fusion Model - Dev Set Results:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\\nAUC-ROC: {auc_roc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hkITxALFmsy",
        "outputId": "7a809b07-2e32-4b51-80d4-4aa6312bf318"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Late Fusion Model - Dev Set Results:\n",
            "Accuracy: 0.5625\n",
            "Precision: 0.5294117647058824\n",
            "Recall: 0.6\n",
            "F1-Score: 0.5625\n",
            "AUC-ROC: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Late fusion / Test set results\n",
        "\n",
        "# Assuming your ImageClassifier class definition is as follows:\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ImageClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the ImageClassifier with appropriate dimensions\n",
        "image_model = ImageClassifier(input_dim=512, hidden_dim=256, output_dim=1)\n",
        "image_representations = torch.load('/content/image_representation.pth')\n",
        "\n",
        "# Set both models to evaluation mode\n",
        "text_model.eval()\n",
        "image_model.eval()\n",
        "\n",
        "# Get predictions from both models\n",
        "with torch.no_grad():\n",
        "    test_text_logits = text_model(test_text_embeddings)\n",
        "    test_image_logits = image_model(test_image_embeddings)\n",
        "\n",
        "    # Average the logits for late fusion\n",
        "    test_combined_logits = (test_text_logits + test_image_logits) / 2\n",
        "    test_combined_preds = torch.sigmoid(test_combined_logits).round().squeeze()\n",
        "\n",
        "# Convert to numpy for sklearn metrics\n",
        "test_combined_preds = test_combined_preds.cpu().numpy()\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "accuracy = accuracy_score(test_labels_np, test_combined_preds)\n",
        "precision = precision_score(test_labels_np, test_combined_preds)\n",
        "recall = recall_score(test_labels_np, test_combined_preds)\n",
        "f1 = f1_score(test_labels_np, test_combined_preds)\n",
        "auc_roc = roc_auc_score(test_labels_np, torch.sigmoid(test_combined_logits).cpu().numpy())\n",
        "\n",
        "print(f\"Late Fusion Model - Test Set Results:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\\nAUC-ROC: {auc_roc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojfEwi_0hYQG",
        "outputId": "bd7524bb-d9f0-4389-fed8-a55b1d39f569"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Late Fusion Model - Test Set Results:\n",
            "Accuracy: 0.54\n",
            "Precision: 0.5849056603773585\n",
            "Recall: 0.5636363636363636\n",
            "F1-Score: 0.5740740740740741\n",
            "AUC-ROC: 0.5208080808080807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-517d006de16a>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  image_representations = torch.load('/content/image_representation.pth')\n"
          ]
        }
      ]
    }
  ]
}
